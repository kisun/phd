\documentclass[11pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{xfrac}

\usepackage{fullpage}
\usepackage{parskip}




\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\bu}{\boldsymbol{u}}
\newcommand{\bw}{\boldsymbol{w}}
\newcommand{\bz}{\boldsymbol{z}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bh}{\boldsymbol{h}}
\newcommand{\bv}{\boldsymbol{v}}
\newcommand{\bfn}{\boldsymbol{f}}
\newcommand{\bF}{\boldsymbol{F}}
\newcommand{\bH}{\boldsymbol{H}}
\newcommand{\bK}{\boldsymbol{K}}
\newcommand{\bQ}{\boldsymbol{Q}}
\newcommand{\bR}{\boldsymbol{R}}
\newcommand{\bP}{\boldsymbol{P}}
\newcommand{\bS}{\boldsymbol{S}}
\newcommand{\bZero}{\boldsymbol{0}}
\newcommand{\dd}[2]{\frac{\partial {#1}}{\partial {#2}}}

\newcommand{\pr}{\mathbb{P}}
\renewcommand{\Pr}[1]{\pr\left(#1\right)}


\newcommand{\km}{_{k-1}}
\newcommand{\kk}{_{k|k}}
\newcommand{\kkm}{_{k|k-1}}
\newcommand{\kmkm}{_{k-1|k-1}}


\title{Kalman Filter}
\author{Overview of models}
\date{}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\maketitle



\section{The Data}

The data come in two parts: GTFS Realtime, for the live location of the bus, and GTFS
Static, for the schedule information (such as route shape, and stop schedule times).  
For now, we ignore the procedures involved in converting GPS location data provided by 
GTFS Realtime into useable data (distance-into-trip), and instead use only the stop arrival
times (and their distances into trip) to get a base speed curve.

An example of the schedule times is here:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.5\textwidth]{figure/stop_times-1} 

}



\end{knitrout}

Obviously, we can't fit a line through points with the same $x$ coordinates, so instead we will only fit the curve
through those with the smallest $y$ values.
We can then use polynomial interpolation to interpolate the scheduled bus path over time:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.5\textwidth]{figure/stop_times_curve-1} 

}



\end{knitrout}




\section{Extended Kalman Filter (EKF) Model}

The state vector is
\begin{equation}
  \label{eq:state-vector}
  \bx_k =
  \begin{bmatrix}
    d_k \\ v_k
  \end{bmatrix}
\end{equation}
where $d_k$ is the distance-into-trip (m) of observation $k$, and $v_k$ is the velocity (m/s) at
observation $k$.


We will be using a nonlinear Kalman Filter to account for scheduled distance traveled in time $\Delta t_k$,
historical information, and a velocity ``penalty'' term:
\begin{equation}
  \label{eq:nonlinKF}
  \bx_k = f\left(\bx_{k-1}, \bu_{k-1}\right) + \bw_{k-1}.
\end{equation}

The control vector, $\bu_k$, will contain additional information used to predict and adjust the estimates of $\bx_k$,
\begin{equation}
  \label{eq:control_vector}
  \bu_k = 
  \begin{bmatrix}
    t_k \\ \Delta t_k
  \end{bmatrix},
\end{equation}
where $t_k$ is the time (in seconds since the trip/block began) of observation $k$, 
and $\Delta t_k$ is the time between observations $k$ and $k+1$.

The difficulty now lies in defining $f$; at first, we can define it as ``the scheduled distance traveled in $\Delta t_k$~s, starting from $d_k$~m into the trip, with a penalty based on the velocity''. For further readability, we define $g = s^{-1}$, which gives us the ``scheduled time'' of a given distance-into-trip:
\begin{align}
  \label{eq:function1}
  \hat d_{k} &= f_d\left(\bx_{k-1},\bu_{k-1}\right) 
  = d_{k-1} + \left[s\left(g\left(d_{k-1}\right) + \Delta t_{k-1}\right) - d_{k-1}\right] \lambda\left(v_{k-1}\right) \\
  \Rightarrow \hat x_{k,1} &= x_{k-1,1} + \left[s\left(g\left(x_{k-1,1}\right) + u_{k-1,2}\right) - x_{k-1,1}\right] \lambda\left(x_{k-1,2}\right) \\
  \intertext{and (for now, assuming a simple velocity function)}
  \hat v_k &= f_v\left(\bx_{k-1},\bu_{k-1}\right) 
  = \frac{s'\left(g\left(d_{k-1}\right) + \Delta t_{k-1}\right)}{s'\left(g\left(d_{k-1}\right)\right)} v_{k-1} \\
  \Rightarrow \hat x_{k,2} &= \frac{s'\left(g\left(x_{k-1,1}\right) + u_{k-1,2}\right)}{s'\left(g\left(x_{k-1,1}\right)\right)} x_{k-1,2}, \\
  \intertext{which gives}
  \hat\bx_k &= \bfn\left(\bx_{k-1},\bu_{k-1}\right) 
  = \begin{bmatrix}
      f_d\left(\bx_{k-1},\bu_{k-1}\right) \\ f_v\left(\bx_{k-1},\bu_{k-1}\right) 
    \end{bmatrix}
\end{align}
However, this needs to be differentiable \ldots we can obtain derivatives of $s$ from the polynomial splines used to create it,
but will this be adequate?

For the Jacobian,
\begin{equation}
  \label{eq:jacobian}
  \bF_k = 
  \begin{bmatrix}
    \frac{\partial\bfn}{\partial x_{k,1}} & \frac{\partial\bfn}{\partial x_{k,2}}_{\hat\bx_{k|k},\bu_k}
  \end{bmatrix},
\end{equation}
which is made up of four partial derivatives (two for each $f$, and dropping the $k$ subscripts for readability):
\begin{align*}
  \dd{f_d}{x_1} &= 1 + \left[s'\left(g\left(x_1\right) + u_2\right)g'\left(x_1\right) - 1\right] \lambda\left(x_2\right) \\
  \dd{f_d}{x_2} &= \left[s\left(g\left(x_1\right) + u_2\right) - x_2\right] \lambda'\left(x_2\right) \\
  \dd{f_v}{x_1} &= \left\{\frac{g'\left(x_1\right) \left[s''\left(g\left(x_1\right) + u_2\right) s'\left(g\left(x_1\right)\right) - s'\left(g\left(x_1\right) + u_2\right) s''\left(g\left(x_1\right)\right)\right]}{\left[s'\left(g\left(x_1\right)\right)\right]^2}\right\} x_2 \\
  \dd{f_v}{x_2} &= \frac{s'\left(g\left(x_1\right) + u_2\right)}{s'\left(g\left(x_1\right)\right)}.
\end{align*}
Fortunately, we only need to be able to evaluate $F$, and the polynomial spline function gives us $s$, $s'$ and $s''$!


For the measurement matrix $\bH$, we use the Jacobian of the function $h$ which maps the predicted state to the observations:
\begin{equation}
  \label{eq:measurements}
  \bz_k = \bh\left(\bx_k\right) + \bv_k,
\end{equation}
where
\begin{align*}
  \bh\left(\bx_k\right) 
  &= 
    \begin{bmatrix}
      x_{k,1} \\ 0
    \end{bmatrix}
\end{align*}
and therefore
\begin{equation*}
  \bH = 
  \begin{bmatrix}
    \dd{h}{x_{k,1}} & \dd{h}{x_{k,2}}
  \end{bmatrix}
  = 
  \begin{bmatrix}
    1 & 0
  \end{bmatrix}
\end{equation*}


We will still need the covariance matrices $\bQ$ and $\bR$, such that:
\begin{align*}
  \bw_{k-1} &\sim \mathcal{N}\left(\bZero, \bQ\right) \\
  \bv_k &\sim \mathcal{N}\left(\bZero, \bR\right).
\end{align*}
This, however, presents an opportunity to use historical data to estimate both $\bQ$ and $\bR$;
this does require historical data, which we don't (yet) have. Based on a set of data we have used (so far),
we will use an estimate of
\begin{equation*}
  \bQ =
  \begin{bmatrix}
    580^2 & -100 \\ -100 & 2.5^2
  \end{bmatrix},
\end{equation*}
and $\bR$, which is essentially the measurement error, we base off of previous studies, $R = \left(150\text{~m}\right)^2$ 
(this is a scalar because the observations are only a single variable: distance-into-trip).


This should be everything we need to implement the following algorithm:

\textbf{Predict}
\begin{align*}
  \intertext{Predicted state estimate}
  \hat\bx\kkm &= \bfn\left(\hat\bx\kmkm, \bu\km\right) \\
  \intertext{Predicted covariance estimate}
  \bP\kkm &= \bF\km \bP\kmkm \bF^T\km + \bQ\km
\end{align*}

\textbf{Update}
\begin{align*}
  \intertext{Innovation or measurement residual}
  \tilde\by_k &= \bz_k - \bh\left(\hat\bx\kkm\right) \\
  \intertext{Innovation (or residual) covariance}
  \bS_k &= \bH_k \bP\kkm \bH^T_k + \bR_k \\
  \intertext{Near-optimal Kalman Gain}
  \bK_k &= \bP\kkm \bH^T_k \bS^{-1}_k \\
  \intertext{Updated covariance estimate}
  \bP\kk &= \left(\boldsymbol{I} - \bK_k \bH_k\right) \bP\kkm \\
  \intertext{Updated state estimate}
  \hat\bx\kk &= \hat\bx\kkm + \bK_k \tilde\by_k
\end{align*}



\section{Example data set}

We obtained a series of GPS observations for a trip.
For the most part, the observations occur every 60~seconds, with some error, and one
occasion where there is a delay of only 10~seconds.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.5\textwidth]{figure/data_set-1} 

}



\end{knitrout}


We now implement the Extended Kalman Filter as described above.


We assume that the initial time and location are 0, although the velocity is unknown, so the initilisation matrices are:
\begin{equation*}
  \mathbf{\hat x}_0 = 
  \begin{bmatrix}
    0 \\ s(0)
  \end{bmatrix}
  \quad \text{and} \quad
  \mathbf{P}_0 =
  \begin{bmatrix}
    0 & 0 \\ 0 & L
  \end{bmatrix}
\end{equation*}
where $L = 100$ is the variation of speed.



Here's the implementation in R:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## observed times}
\hlstd{t} \hlkwb{<-} \hlstd{dat}\hlopt{$}\hlstd{T}
\hlcom{## observed distances}
\hlstd{d} \hlkwb{<-} \hlstd{dat}\hlopt{$}\hlstd{dist}

\hlcom{## the schedule and inverse-schedule functions}
\hlstd{st} \hlkwb{<-} \hlkwd{unique}\hlstd{(sched}\hlopt{$}\hlstd{time)}
\hlstd{sd} \hlkwb{<-} \hlkwd{tapply}\hlstd{(sched}\hlopt{$}\hlstd{shape_dist_traveled, sched}\hlopt{$}\hlstd{time, min)}

\hlstd{s} \hlkwb{<-} \hlkwd{splinefun}\hlstd{(st, sd)}
\hlstd{g} \hlkwb{<-} \hlkwd{splinefun}\hlstd{(sd, st)}

\hlcom{## prediction function}
\hlstd{lambda} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{...}\hlstd{)} \hlnum{1} \hlcom{## for now, do nothing ...}
\hlstd{lambda.} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{...}\hlstd{)} \hlnum{0}

\hlstd{f} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{,} \hlkwc{u}\hlstd{)}
    \hlkwd{c}\hlstd{(x[}\hlnum{1}\hlstd{]} \hlopt{+} \hlstd{(}\hlkwd{s}\hlstd{(}\hlkwd{g}\hlstd{(x[}\hlnum{1}\hlstd{])} \hlopt{+} \hlstd{u[}\hlnum{2}\hlstd{])} \hlopt{-} \hlstd{x[}\hlnum{1}\hlstd{])} \hlopt{*} \hlkwd{lambda}\hlstd{(x[}\hlnum{2}\hlstd{]),}
      \hlkwd{s}\hlstd{(}\hlkwd{g}\hlstd{(x[}\hlnum{1}\hlstd{])} \hlopt{+} \hlstd{u[}\hlnum{2}\hlstd{],} \hlnum{1}\hlstd{)} \hlopt{/} \hlkwd{s}\hlstd{(}\hlkwd{g}\hlstd{(x[}\hlnum{1}\hlstd{]),} \hlnum{1}\hlstd{)} \hlopt{*} \hlstd{x[}\hlnum{2}\hlstd{])}

\hlcom{## and it's jacobian:}
\hlstd{F} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{,} \hlkwc{u}\hlstd{) \{}
    \hlstd{f1x1} \hlkwb{<-} \hlnum{1} \hlopt{+} \hlstd{(}\hlkwd{s}\hlstd{(}\hlkwd{g}\hlstd{(x[}\hlnum{1}\hlstd{])} \hlopt{+} \hlstd{u[}\hlnum{2}\hlstd{],} \hlnum{1}\hlstd{)} \hlopt{*} \hlkwd{g}\hlstd{(x[}\hlnum{1}\hlstd{],} \hlnum{1}\hlstd{)} \hlopt{-} \hlnum{1}\hlstd{)} \hlopt{*} \hlkwd{lambda}\hlstd{(x[}\hlnum{2}\hlstd{])}
    \hlstd{f1x2} \hlkwb{<-} \hlstd{(}\hlkwd{s}\hlstd{(}\hlkwd{g}\hlstd{(x[}\hlnum{1}\hlstd{])} \hlopt{+} \hlstd{u[}\hlnum{2}\hlstd{])} \hlopt{-} \hlstd{x[}\hlnum{2}\hlstd{])} \hlopt{*} \hlkwd{lambda.}\hlstd{(x[}\hlnum{2}\hlstd{])}

    \hlstd{f2x1} \hlkwb{<-} \hlstd{((}\hlkwd{g}\hlstd{(x[}\hlnum{1}\hlstd{],} \hlnum{1}\hlstd{)} \hlopt{*} \hlstd{(}\hlkwd{s}\hlstd{(}\hlkwd{g}\hlstd{(x[}\hlnum{1}\hlstd{])} \hlopt{+} \hlstd{u[}\hlnum{2}\hlstd{],} \hlnum{2}\hlstd{)} \hlopt{*} \hlkwd{s}\hlstd{(}\hlkwd{g}\hlstd{(x[}\hlnum{1}\hlstd{]),} \hlnum{1}\hlstd{)} \hlopt{-} \hlkwd{s}\hlstd{(}\hlkwd{g}\hlstd{(x[}\hlnum{1}\hlstd{])} \hlopt{+} \hlstd{u[}\hlnum{2}\hlstd{],} \hlnum{1}\hlstd{)} \hlopt{*} \hlkwd{s}\hlstd{(}\hlkwd{g}\hlstd{(x[}\hlnum{1}\hlstd{]),} \hlnum{2}\hlstd{)))} \hlopt{/} \hlstd{(}\hlkwd{s}\hlstd{(}\hlkwd{g}\hlstd{(x[}\hlnum{1}\hlstd{]),} \hlnum{1}\hlstd{)}\hlopt{^}\hlnum{2}\hlstd{))} \hlopt{*} \hlstd{x[}\hlnum{2}\hlstd{]}
    \hlstd{f2x2} \hlkwb{<-} \hlkwd{s}\hlstd{(}\hlkwd{g}\hlstd{(x[}\hlnum{1}\hlstd{])} \hlopt{+} \hlstd{u[}\hlnum{2}\hlstd{],} \hlnum{1}\hlstd{)} \hlopt{/} \hlkwd{s}\hlstd{(}\hlkwd{g}\hlstd{(x[}\hlnum{1}\hlstd{]),} \hlnum{1}\hlstd{)}

    \hlkwd{rbind}\hlstd{(}\hlkwd{c}\hlstd{(f1x1, f1x2),} \hlkwd{c}\hlstd{(f2x1, f2x2))}
\hlstd{\}}

\hlcom{## Measurement matrix:}
\hlstd{h} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{rbind}\hlstd{(x[}\hlnum{1}\hlstd{],} \hlnum{0}\hlstd{)}
\hlstd{H} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)} \hlkwd{cbind}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{0}\hlstd{)}

\hlcom{## Variance matrices:}
\hlstd{Q} \hlkwb{<-} \hlkwd{rbind}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{580}\hlopt{^}\hlnum{2}\hlstd{,} \hlopt{-}\hlnum{100}\hlstd{),}
           \hlkwd{c}\hlstd{(}\hlopt{-}\hlnum{100}\hlstd{,} \hlnum{2.5}\hlopt{^}\hlnum{2}\hlstd{))}
\hlstd{R} \hlkwb{<-} \hlnum{250}\hlopt{^}\hlnum{2}

\hlcom{## And the initialisation:}
\hlstd{X.} \hlkwb{<-} \hlstd{X} \hlkwb{<-} \hlkwd{array}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwd{c}\hlstd{(}\hlnum{2}\hlstd{,} \hlkwd{length}\hlstd{(x)))}
\hlstd{X[,} \hlnum{1}\hlstd{]} \hlkwb{<-} \hlkwd{rbind}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwd{s}\hlstd{(}\hlnum{0}\hlstd{))}
\hlstd{P.} \hlkwb{<-} \hlstd{P} \hlkwb{<-} \hlkwd{array}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwd{c}\hlstd{(}\hlnum{2}\hlstd{,} \hlnum{2}\hlstd{,} \hlkwd{length}\hlstd{(x)))}
\hlstd{P[,,}\hlnum{1}\hlstd{]} \hlkwb{<-} \hlkwd{rbind}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{0}\hlstd{),} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{100}\hlstd{))}


\hlcom{## and run it ...}

\hlstd{EKF} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{d}\hlstd{) \{}
    \hlcom{## input d is a list of :}
    \hlcom{## xhat  the estimate of \textbackslash{}hat x_\{k-1\}}
    \hlcom{## Phat  the estimate of \textbackslash{}hat P_\{k-1\}}
    \hlcom{## z     the data z_k}
    \hlcom{## u     the control variables u_\{k-1\}}

    \hlcom{## return:}
    \hlcom{## x.  the a priori estimate of x_k}
    \hlcom{## P.  the a priori estimate of covariance}
    \hlcom{## xn  the a posteriori estimate of x_k}
    \hlcom{## Pn  the a posteriori estimate of covariance}

    \hlcom{## predict}
    \hlstd{x.} \hlkwb{<-} \hlkwd{f}\hlstd{(d}\hlopt{$}\hlstd{xhat, d}\hlopt{$}\hlstd{u)}
    \hlstd{Fk} \hlkwb{<-} \hlkwd{F}\hlstd{(d}\hlopt{$}\hlstd{xhat, d}\hlopt{$}\hlstd{u)}
    \hlstd{P.} \hlkwb{<-} \hlstd{Fk} \hlopt{%*%} \hlstd{d}\hlopt{$}\hlstd{Phat} \hlopt{%*%} \hlkwd{t}\hlstd{(Fk)} \hlopt{+} \hlstd{Q} \hlopt{*} \hlstd{(d}\hlopt{$}\hlstd{u[}\hlnum{2}\hlstd{]} \hlopt{/} \hlnum{60}\hlstd{)}

    \hlcom{## update}
    \hlkwa{if} \hlstd{(}\hlstr{"z"} \hlopt{%in%} \hlkwd{names}\hlstd{(d)) \{}
        \hlstd{yk} \hlkwb{<-} \hlstd{d}\hlopt{$}\hlstd{z} \hlopt{-} \hlkwd{h}\hlstd{(x.)}

        \hlstd{Hk} \hlkwb{<-} \hlkwd{H}\hlstd{(x.)}
        \hlstd{Sk} \hlkwb{<-} \hlstd{Hk} \hlopt{%*%} \hlstd{P.} \hlopt{%*%} \hlkwd{t}\hlstd{(Hk)} \hlopt{+} \hlstd{R}

        \hlstd{Kk} \hlkwb{<-} \hlstd{P.} \hlopt{%*%} \hlkwd{t}\hlstd{(Hk)} \hlopt{%*%} \hlkwd{solve}\hlstd{(Sk)}

        \hlstd{Pn} \hlkwb{<-} \hlstd{(}\hlkwd{diag}\hlstd{(}\hlnum{2}\hlstd{)} \hlopt{-} \hlstd{Kk} \hlopt{%*%} \hlstd{Hk)} \hlopt{*} \hlstd{P.}

        \hlstd{xn} \hlkwb{<-} \hlstd{x.} \hlopt{+} \hlstd{Kk} \hlopt{%*%} \hlstd{yk[}\hlnum{1}\hlstd{]}
    \hlstd{\}} \hlkwa{else} \hlstd{\{}
        \hlstd{Pn} \hlkwb{<-} \hlstd{P.}
        \hlstd{xn} \hlkwb{<-} \hlstd{x.}
    \hlstd{\}}

    \hlkwd{list}\hlstd{(}\hlkwc{x.} \hlstd{= x.,} \hlkwc{P.} \hlstd{= P.,} \hlkwc{xn} \hlstd{= xn,} \hlkwc{Pn} \hlstd{= Pn)}
\hlstd{\}}

\hlkwa{for} \hlstd{(k} \hlkwa{in} \hlnum{2}\hlopt{:}\hlkwd{length}\hlstd{(x)) \{}
    \hlstd{d} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{xhat} \hlstd{= X[, k}\hlopt{-}\hlnum{1}\hlstd{],} \hlkwc{Phat} \hlstd{= P[,,k}\hlopt{-}\hlnum{1}\hlstd{],} \hlkwc{z} \hlstd{=} \hlkwd{c}\hlstd{(x[k],} \hlnum{0}\hlstd{),} \hlkwc{u} \hlstd{=} \hlkwd{c}\hlstd{(t[k}\hlopt{-}\hlnum{1}\hlstd{],} \hlkwd{diff}\hlstd{(t)[k}\hlopt{-}\hlnum{1}\hlstd{]))}
    \hlstd{KF} \hlkwb{<-} \hlkwd{EKF}\hlstd{(d)}
    \hlstd{X.[, k]} \hlkwb{<-} \hlstd{KF}\hlopt{$}\hlstd{x.}
    \hlstd{P.[,,k]} \hlkwb{<-} \hlstd{KF}\hlopt{$}\hlstd{P.}
    \hlstd{X[, k]} \hlkwb{<-} \hlstd{KF}\hlopt{$}\hlstd{xn}
    \hlstd{P[,,k]} \hlkwb{<-} \hlstd{KF}\hlopt{$}\hlstd{Pn}
\hlstd{\}}

\hlkwd{plot}\hlstd{(t, x,} \hlkwc{type} \hlstd{=} \hlstr{"n"}\hlstd{,} \hlkwc{xlim} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{700}\hlstd{),} \hlkwc{ylim} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwd{s}\hlstd{(}\hlnum{700}\hlstd{)))}
\hlkwd{curve}\hlstd{(}\hlkwd{s}\hlstd{(x),} \hlnum{0}\hlstd{,} \hlnum{700}\hlstd{,} \hlkwc{add}\hlstd{=T,} \hlkwc{col}\hlstd{=}\hlstr{"red"}\hlstd{)}
\hlkwd{lines}\hlstd{(t, X[}\hlnum{1}\hlstd{, ],} \hlkwc{col} \hlstd{=} \hlstr{"blue"}\hlstd{,} \hlkwc{cex} \hlstd{=} \hlnum{0.5}\hlstd{,} \hlkwc{pch} \hlstd{=} \hlnum{19}\hlstd{)}
\hlstd{xhat.} \hlkwb{<-} \hlstd{X.[}\hlnum{1}\hlstd{, ]}
\hlstd{xhatSD.} \hlkwb{<-} \hlkwd{sqrt}\hlstd{(P[}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{,])}
\hlstd{ci.l} \hlkwb{<-} \hlkwd{qnorm}\hlstd{(}\hlnum{0.025}\hlstd{, xhat., xhatSD.)}
\hlstd{ci.u} \hlkwb{<-} \hlkwd{qnorm}\hlstd{(}\hlnum{0.975}\hlstd{, xhat., xhatSD.)}
\hlkwd{arrows}\hlstd{(t, ci.l, t, ci.u,} \hlkwc{length} \hlstd{=} \hlnum{0.05}\hlstd{,} \hlkwc{angle} \hlstd{=} \hlnum{90}\hlstd{,} \hlkwc{code} \hlstd{=} \hlnum{3}\hlstd{,} \hlkwc{col} \hlstd{=} \hlstr{"#00880060"}\hlstd{)}
\hlkwd{points}\hlstd{(t, xhat.,} \hlkwc{pch} \hlstd{=} \hlnum{21}\hlstd{,} \hlkwc{bg} \hlstd{=} \hlstr{"white"}\hlstd{,} \hlkwc{col} \hlstd{=} \hlstr{"#00880060"}\hlstd{)}
\hlkwd{points}\hlstd{(t, x,} \hlkwc{cex} \hlstd{=} \hlnum{0.4}\hlstd{,} \hlkwc{pch} \hlstd{=} \hlnum{19}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/kalman_filter-1} 

\end{knitrout}


It looks like the bus was delayed starting its route, but after that it has no delays. Lets look at predicting the arrival time at the second-to-last stop.


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.8\textwidth]{figure/unnamed-chunk-2-1} 

}



\end{knitrout}

After the first few EKF iterations, we can predict into the future?

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.8\textwidth]{figure/unnamed-chunk-3-1} 

}




{\centering \includegraphics[width=0.8\textwidth]{figure/unnamed-chunk-3-2} 

}




{\centering \includegraphics[width=0.8\textwidth]{figure/unnamed-chunk-3-3} 

}




{\centering \includegraphics[width=0.8\textwidth]{figure/unnamed-chunk-3-4} 

}



\end{knitrout}




So, we now have a posterior distribution of where the bus will be at time $t_j = t_k + \delta_j$.
We next need to convert this to a distribution of \emph{when} the bus will be at distance $d_i$
(i.e., when will it arrive at stop $i$?).

Using Bayes' Rule:
\begin{align*}
   \Pr{t = t_j | d_k = d_i}
  &= \frac{\Pr{d_k = d_i | t = t_j} \Pr{t = t_j}}{\Pr{d_k = d_i}} \\
  &= \frac{\Pr{d_k = d_i | t = t_j} \Pr{t = t_j}}{\int\Pr{d_k = d_i | t = t_j} \Pr{t = t_j}\,\mathrm{d}t} \\
  \intertext{which, for the case where we compute discrete instances of $t$, is}
  &= \frac{\Pr{d_k = d_i | t = t_j} \Pr{t = t_j}}{\sum_j\Pr{d_k = d_i | t = t_j} \Pr{t = t_j}\,\mathrm{d}t} \\
\end{align*}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{stop} \hlkwb{<-} \hlkwd{plotMe}\hlstd{(}\hlnum{FALSE}\hlstd{)}
\hlstd{di} \hlkwb{<-} \hlstd{stop}\hlopt{$}\hlstd{shape_dist_traveled}

\hlstd{ff} \hlkwb{<-} \hlkwd{futureSight}\hlstd{(}\hlnum{20}\hlstd{,} \hlnum{500}\hlstd{)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=0.8\textwidth]{figure/posterior_arrival-1} 

}


\begin{kframe}\begin{alltt}
\hlstd{Pdj} \hlkwb{<-} \hlkwd{dnorm}\hlstd{(di, ff}\hlopt{$}\hlstd{mean, ff}\hlopt{$}\hlstd{sd)}
\hlstd{Pd} \hlkwb{<-} \hlkwd{sum}\hlstd{(Pdj)}

\hlkwd{plot}\hlstd{(ff}\hlopt{$}\hlstd{t, Pdj} \hlopt{/} \hlstd{Pd,} \hlkwc{type} \hlstd{=} \hlstr{"l"}\hlstd{,} \hlkwc{xlab} \hlstd{=} \hlstr{"Time (s)"}\hlstd{,} \hlkwc{ylab} \hlstd{=} \hlstr{"Pr(D = d | T = t)"}\hlstd{)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=0.8\textwidth]{figure/posterior_arrival-2} 

}



\end{knitrout}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{stop} \hlkwb{<-} \hlkwd{plotMe}\hlstd{(}\hlnum{FALSE}\hlstd{)}
\hlstd{di} \hlkwb{<-} \hlstd{stop}\hlopt{$}\hlstd{shape_dist_traveled}

\hlstd{ff} \hlkwb{<-} \hlkwd{futureSight}\hlstd{(}\hlnum{10}\hlstd{,} \hlnum{500}\hlstd{)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=0.8\textwidth]{figure/posterior_arrival2-1} 

}


\begin{kframe}\begin{alltt}
\hlstd{Pdj} \hlkwb{<-} \hlkwd{dnorm}\hlstd{(di, ff}\hlopt{$}\hlstd{mean, ff}\hlopt{$}\hlstd{sd)}
\hlstd{Pd} \hlkwb{<-} \hlkwd{sum}\hlstd{(Pdj)}

\hlkwd{plot}\hlstd{(ff}\hlopt{$}\hlstd{t, Pdj} \hlopt{/} \hlstd{Pd,} \hlkwc{type} \hlstd{=} \hlstr{"l"}\hlstd{,} \hlkwc{xlab} \hlstd{=} \hlstr{"Time (s)"}\hlstd{,} \hlkwc{ylab} \hlstd{=} \hlstr{"Pr(D = d | T = t)"}\hlstd{)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=0.8\textwidth]{figure/posterior_arrival2-2} 

}



\end{knitrout}


Look at the ginormous interval!
Hopefully if we use better information about the variance etc, we can get something a tad more precise.




\section{EFK kinda sucks\ldots Let's try Particle Filter!}


We will be trying the \textbf{Bayesian bootstrap filter} next, which doesn't assume linearity or normality.
For this, we need:

\begin{enumerate}
\item the distrbution of $p\left(x_0\right)$ available for sampling;
  
  
\item the likelihood, $p\left(y_k | x_k\right)$, to be a known functional form;
  
  
\item the noise distribution, $p\left(w_k\right)$ available for sampling.
  
  
\end{enumerate}



\subsubsection*{1. The distribution of $p\left(x_0\right)$}

This is the initial state, which we defined earlier as
\begin{equation}
  \label{eq:initial_state}
  \bx_0 \sim \mathcal{N}\left(
    \begin{bmatrix}
      0 \\ s(0)
    \end{bmatrix},
    \begin{bmatrix}
      0 & 0 \\ 0 & L
    \end{bmatrix}
  \right),
  \quad\quad L = 100,
\end{equation}
which is easy to sample.



\subsubsection*{2. The Likelihood, $p\left(y_k | x_k\right)$}

This, too, was defined earlier without the noise term; we will simply assume a linear noise term:
\begin{equation}
  \label{eq:likelihood_pf}
  p\left(y_k | \bx_k\right) = \bx_k +  v_{k},\quad
  v_k \sim \mathcal{N}\left(0,R\right),\quad
  R = 250^2
\end{equation}



\subsubsection*{3. The noise distribution,  $p\left(w_k\right)$}

Earlier, we just used
\begin{equation}
  \label{eq:system_noise}
  \bw_k \sim \mathcal{N}\left(\bZero, \bQ\right),
  \quad\quad 
  \bQ = 
  \begin{bmatrix}
    580^2 & -100 \\ -100 & 2.5^2
  \end{bmatrix}.
\end{equation}
This will do for now, until we have historical data to replace it with.




The algorithm is broken up into two parts (as usual): predict and update.

\subsection{Predict}

\begin{equation}
  \label{eq:predict_step}
  \bx_k^\star(i) =  \bfn\left(\bx_{k-1}(i), \bu_{k-1} \right) +  \bw_{k-1}(i),\quad
  \bw_{k-1}(i) \sim p\left(\bw_{k-1}\right),\quad
  i = 1, \ldots, N
\end{equation}


\subsection{Update}

Using the observed data, $y_k$, we calculate a weight for each sample:
\begin{equation}
  \label{eq:normalisation_weight}
  q_i = \frac{
    p\left(y_k | \bx_k^\star (i)\right)
  }{
    \sum_{j = 1}^N p\left(y_k | \bx_k^\star (j)\right)
  },
\end{equation}
which are used as the probability masses for each element $i$ of the discrete distribution defined over 
$\left\{\bx_k^\star (i): i = 1,\ldots,N\right\}$.
The ``bootstrap'' occurs by taking $N$ samples from this distribution using the probabilities
\begin{equation}
  \label{eq:boot_probs}
  \Pr{\bx_k(j) = \bx_k^\star(i)} = q_i.
\end{equation}


And that's it! To start the algorithm, we just take $N$ samples $\bx_0^\star(i)$ from
$p\left(x_0\right)$ that we defined above.



\subsection{Implementation}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{require}\hlstd{(mvtnorm)}
\hlstd{y} \hlkwb{<-} \hlstd{dat}\hlopt{$}\hlstd{dist}  \hlcom{## the observations}
\hlstd{u} \hlkwb{<-} \hlkwd{cbind}\hlstd{(dat}\hlopt{$}\hlstd{time,} \hlkwd{c}\hlstd{(}\hlkwd{diff}\hlstd{(dat}\hlopt{$}\hlstd{time),} \hlnum{0}\hlstd{))}  \hlcom{## the control vector}

\hlstd{F2} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{,} \hlkwc{u}\hlstd{,} \hlkwc{w}\hlstd{)}
    \hlkwd{f}\hlstd{(x, u)} \hlopt{+} \hlstd{w}

\hlstd{PrY.X} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{y}\hlstd{,} \hlkwc{x}\hlstd{,} \hlkwc{R} \hlstd{=} \hlnum{250}\hlstd{)}
    \hlkwd{dnorm}\hlstd{(y, x, R)}

\hlstd{qs} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{y}\hlstd{,} \hlkwc{x}\hlstd{) \{}
    \hlstd{pi} \hlkwb{<-} \hlkwd{PrY.X}\hlstd{(y, x)}
    \hlkwd{sapply}\hlstd{(pi,} \hlkwa{function}\hlstd{(}\hlkwc{p}\hlstd{) p} \hlopt{/} \hlkwd{sum}\hlstd{(pi))}
\hlstd{\}}

\hlstd{boot} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{,} \hlkwc{q}\hlstd{,} \hlkwc{N}\hlstd{) \{}
    \hlstd{w} \hlkwb{<-} \hlkwd{sample}\hlstd{(}\hlnum{1}\hlopt{:}\hlstd{N, N,} \hlnum{TRUE}\hlstd{, q)}
    \hlstd{x[, w]}
\hlstd{\}}

\hlstd{N} \hlkwb{<-} \hlnum{500}

\hlcom{## Initialise}

\hlstd{X.} \hlkwb{<-} \hlstd{X} \hlkwb{<-} \hlkwd{array}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwd{c}\hlstd{(}\hlnum{2}\hlstd{, N,} \hlkwd{length}\hlstd{(y)))}
\hlstd{x0} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwd{s}\hlstd{(}\hlnum{0}\hlstd{))}
\hlstd{P0} \hlkwb{<-} \hlkwd{rbind}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{0}\hlstd{),} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{100}\hlstd{))}

\hlstd{X[, ,} \hlnum{1}\hlstd{]} \hlkwb{<-} \hlkwd{t}\hlstd{(}\hlkwd{rmvnorm}\hlstd{(N, x0, P0))}


\hlcom{## Run}

\hlkwa{for} \hlstd{(k} \hlkwa{in} \hlnum{2}\hlopt{:}\hlkwd{length}\hlstd{(y)) \{}
    \hlcom{## Predict}
    \hlstd{wi} \hlkwb{<-} \hlkwd{t}\hlstd{(}\hlkwd{rmvnorm}\hlstd{(N,} \hlkwc{sigma} \hlstd{= Q))}
    \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{N) \{}
        \hlstd{X.[, i, k]} \hlkwb{<-} \hlkwd{F2}\hlstd{(X[, i, k} \hlopt{-} \hlnum{1}\hlstd{], u[k} \hlopt{-} \hlnum{1}\hlstd{, ], wi[, i])}
    \hlstd{\}}

    \hlcom{## Update}
    \hlstd{qi} \hlkwb{<-} \hlkwd{qs}\hlstd{(y[k], X.[}\hlnum{1}\hlstd{,,k])}
    \hlstd{X[,,k]} \hlkwb{<-} \hlkwd{boot}\hlstd{(X.[,,k], qi, N)}
\hlstd{\}}

\hlkwd{plot}\hlstd{(}\hlkwd{matrix}\hlstd{(}\hlkwd{rep}\hlstd{(t[}\hlopt{-}\hlnum{1}\hlstd{], N),} \hlkwc{nrow}\hlstd{=N,} \hlkwc{byrow}\hlstd{=T), X.[}\hlnum{1}\hlstd{,,}\hlopt{-}\hlnum{1}\hlstd{],} \hlkwc{pch} \hlstd{=} \hlnum{19}\hlstd{,} \hlkwc{cex} \hlstd{=} \hlnum{0.5}\hlstd{,}
     \hlkwc{ylim} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwd{max}\hlstd{(}\hlkwd{s}\hlstd{(t), X[}\hlnum{1}\hlstd{,,], X.[}\hlnum{1}\hlstd{,,}\hlopt{-}\hlnum{1}\hlstd{])),} \hlkwc{col} \hlstd{=} \hlstr{"#00000030"}\hlstd{,}
     \hlkwc{xlab} \hlstd{=} \hlstr{"Time (s)"}\hlstd{,} \hlkwc{ylab} \hlstd{=} \hlstr{"Distance into Trip (m)"}\hlstd{)}
\hlkwd{points}\hlstd{(}\hlkwd{matrix}\hlstd{(}\hlkwd{rep}\hlstd{(t, N),} \hlkwc{nrow}\hlstd{=N,} \hlkwc{byrow}\hlstd{=T), X[}\hlnum{1}\hlstd{,,],} \hlkwc{pch} \hlstd{=} \hlnum{19}\hlstd{,} \hlkwc{cex} \hlstd{=} \hlnum{0.2}\hlstd{,} \hlkwc{col} \hlstd{=} \hlstr{"red"}\hlstd{)}
\hlkwd{points}\hlstd{(t, y,} \hlkwc{pch} \hlstd{=} \hlnum{19}\hlstd{,} \hlkwc{col} \hlstd{=} \hlstr{"white"}\hlstd{,} \hlkwc{cex} \hlstd{=} \hlnum{0.5}\hlstd{)}
\hlkwd{curve}\hlstd{(}\hlkwd{s}\hlstd{(x),} \hlnum{0}\hlstd{,} \hlkwd{max}\hlstd{(t),} \hlnum{1001}\hlstd{,} \hlkwc{add} \hlstd{=} \hlnum{TRUE}\hlstd{,} \hlkwc{col} \hlstd{=} \hlstr{"blue"}\hlstd{)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=0.8\textwidth]{figure/particle_filter-1} 

}



\end{knitrout}


\end{document}
